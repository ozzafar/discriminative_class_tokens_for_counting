{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100",
   "collapsed_sections": [
    "Ki8KIdXG3VG8",
    "DRWHrWCOQtHS"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e323fc445422434a88336c4eeb5d423b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b79a3b4a2ed84b15828ad59b029a5fdd",
       "IPY_MODEL_d2fc80860b234546bc5c0067c585fe91",
       "IPY_MODEL_2c1bcd22eedd4c67be4e962005482dfd"
      ],
      "layout": "IPY_MODEL_36b4457dc0614826882aa714f02dcfe4"
     }
    },
    "b79a3b4a2ed84b15828ad59b029a5fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb6a1537f6f1401a9d95f8f665440daf",
      "placeholder": "​",
      "style": "IPY_MODEL_685f8f88ebed426f97a8d1a3c3799b5e",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "d2fc80860b234546bc5c0067c585fe91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_874f536d05074145af6a1f866ccfc80d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b71b767e97e844c1bd3aed1c3a9a456e",
      "value": 7
     }
    },
    "2c1bcd22eedd4c67be4e962005482dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_379887bc2d3b4547a9bca6fea6071db2",
      "placeholder": "​",
      "style": "IPY_MODEL_094033e01e174fd1881f64d4f5359f00",
      "value": " 7/7 [00:01&lt;00:00,  6.59it/s]"
     }
    },
    "36b4457dc0614826882aa714f02dcfe4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb6a1537f6f1401a9d95f8f665440daf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "685f8f88ebed426f97a8d1a3c3799b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "874f536d05074145af6a1f866ccfc80d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b71b767e97e844c1bd3aed1c3a9a456e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "379887bc2d3b4547a9bca6fea6071db2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "094033e01e174fd1881f64d4f5359f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Setup (may take a few minutes)\n",
    "import os\n",
    "\n",
    "\n",
    "# !pip install accelerate\n",
    "# !pip install diffusers\n",
    "# !pip install transformers\n",
    "# !pip install kornia==0.6.11\n",
    "# !pip install pyrallis==0.3.1\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.utils.checkpoint\n",
    "import itertools\n",
    "from accelerate import Accelerator\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import prompt_dataset\n",
    "import utils\n",
    "from inet_classes import IDX2NAME as IDX2NAME_INET\n",
    "\n",
    "import shutil\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "#Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Hyperparameters"
   ],
   "metadata": {
    "id": "cm-dEEZDQ6Ba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier: str = 'clip-count' #@param ['inet', 'inat', 'cub','clip','clip-count'] {type:\"string\"}\n",
    "\n",
    "_lambda: float = 1  #@param {type:\"number\"}\n",
    "scale: float = 70 # 80  #@param {type:\"number\"}\n",
    "\n",
    "# Affect training time\n",
    "early_stopping: int = 15 #@param {type:\"integer\"}\n",
    "num_train_epochs: int = 50 #@param {type:\"integer\"}\n",
    "\n",
    "# affect variability of the training images\n",
    "# i.e., also sets batch size with accumulation\n",
    "epoch_size: int = 1 #@param {type:\"integer\"}\n",
    "number_of_prompts: int = 1 #@param {type:\"integer\"}\n",
    "batch_size: int = 1 #@param {type:\"integer\"}\n",
    "gradient_accumulation_steps: int = 1 #@param {type:\"integer\"}\n",
    "\n",
    "# Skip if there exists a token checkpoint\n",
    "skip_exists: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Train and Optimization\n",
    "lr: float = 0.00125  #@param {type:\"number\"}\n",
    "betas1: tuple = 0.9 #@param {type:\"number\"}\n",
    "betas2: tuple = 0.999 #@param {type:\"number\"}\n",
    "betas = (betas1, betas2)\n",
    "\n",
    "\n",
    "weight_decay: float = 1e-2 #@param {type:\"number\"}\n",
    "eps: float = 1e-08 #@param {type:\"number\"}\n",
    "max_grad_norm: str = \"1\" #@param {type:\"string\"}\n",
    "seed: int = 35 #@param {type:\"integer\"}\n",
    "\n",
    "# Generative model\n",
    "guidance_scale: int = 7 #@param {type:\"integer\"}\n",
    "height: int = 512 #@param {type:\"integer\"}\n",
    "width: int = 512 #@param {type:\"integer\"}\n",
    "num_of_SD_inference_steps: int = 35 #@param {type:\"integer\"}\n",
    "num_of_SD_backpropagation_steps: int = 1 #@param {type:\"integer\"}\n",
    "\n",
    "# Discriminative tokens\n",
    "placeholder_token: str = \"newclas\" #@param {type:\"string\"}\n",
    "initializer_token: str = \"a\" #@param {type:\"string\"}\n",
    "\n",
    "# Path to save all outputs to\n",
    "output_path: str = \"results\" #@param {type:\"string\"}\n",
    "save_as_full_pipeline: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Cuda related\n",
    "device: str = \"cuda\" #@param {type:\"string\"}\n",
    "mixed_precision: str = \"no\" #@param [\"fp16\", \"fp32\"] {type:\"string\"}\n",
    "gradient_checkpointing: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# evaluate\n",
    "test_size: int = 3 #@param {type:\"integer\"}"
   ],
   "metadata": {
    "id": "uY7BoAjxz5LD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "# Define the configuration names\n",
    "config_names = [\n",
    "    \"classifier\",\n",
    "    \"early_stopping\",\n",
    "    \"num_train_epochs\",\n",
    "    \"epoch_size\",\n",
    "    \"number_of_prompts\",\n",
    "    \"batch_size\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"skip_exists\",\n",
    "    \"betas\",\n",
    "    \"lr\",\n",
    "    \"eps\",\n",
    "    \"weight_decay\",\n",
    "    \"seed\",\n",
    "    \"max_grad_norm\",\n",
    "    \"guidance_scale\",\n",
    "    \"height\",\n",
    "    \"width\",\n",
    "    \"num_of_SD_inference_steps\",\n",
    "    \"num_of_SD_backpropagation_steps\",\n",
    "    \"placeholder_token\",\n",
    "    \"initializer_token\",\n",
    "    \"output_path\",\n",
    "    \"save_as_full_pipeline\",\n",
    "    \"device\",\n",
    "    \"mixed_precision\",\n",
    "    \"gradient_checkpointing\",\n",
    "    \"test_size\",\n",
    "    \"scale\"\n",
    "]\n",
    "\n",
    "# Use globals() to extract values from matching variable names\n",
    "config_values = [globals()[name] for name in config_names]\n",
    "\n",
    "# Create the named tuple\n",
    "Config = namedtuple(\"Config\", config_names)\n",
    "config = Config(*config_values)\n",
    "config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR2klWJI9c-a",
    "outputId": "614c9805-210c-40db-d56b-e0d576faf618"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "id": "Ki8KIdXG3VG8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from insta_flow.code.pipeline_rf import RectifiedFlowPipeline\n",
    "# \n",
    "# generator = torch.Generator(\n",
    "#     device=config.device\n",
    "# )  # Seed generator to create the inital latent noise\n",
    "# \n",
    "# generator.manual_seed(config.seed)\n",
    "# \n",
    "# \n",
    "# pipeline = RectifiedFlowPipeline.from_pretrained(\n",
    "#     \"XCLIU/instaflow_0_9B_from_sd_1_5\",\n",
    "#     safety_checker = None,\n",
    "#     torch_dtype=torch.float32\n",
    "# ).to(device)\n",
    "# \n",
    "# # generate image    \n",
    "# image = pipeline(prompt=\"a photo of 7 oranges\",\n",
    "#     num_inference_steps=1,\n",
    "#     generator=generator,\n",
    "#     guidance_scale=0.0,\n",
    "#     output_type=\"pil\"\n",
    "# ).images[0] \n",
    "# \n",
    "# image.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def is_valid(matrix, row, col, visited):\n",
    "        num_rows = len(matrix)\n",
    "        num_cols = len(matrix[0])\n",
    "        return (row >= 0 and row < num_rows and col >= 0 and col < num_cols and matrix[row][col] != 0 and not visited[row][col])\n",
    "\n",
    "def dfs_rec(matrix, row, col, visited):\n",
    "    \n",
    "    if not visited[row][col]:\n",
    "        matrix[row][col] = 0\n",
    "\n",
    "    visited[row][col] = True\n",
    "    \n",
    "    steps = [0,1,-1]\n",
    "    # Perform DFS on the neighbors\\n\",\n",
    "    for i in steps:\n",
    "        for j in steps:\n",
    "            if is_valid(matrix, row + i, col + j, visited):\n",
    "                matrix[row+i][col+j] = 0\n",
    "                dfs_rec(matrix, row + i, col + j, visited)\n",
    "\n",
    "def dfs(matrix):\n",
    "    num_rows = len(matrix)\n",
    "    num_cols = len(matrix[0])\n",
    "\n",
    "    visited = [[False] * num_cols for _ in range(num_rows)]\n",
    "\n",
    "    # Traverse the matrix\\n\",\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            if matrix[i][j] > 0.9:\n",
    "                visited[i][j] = True\n",
    "                dfs_rec(matrix, i, j, visited)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from insta_flow.code.pipeline_rf import RectifiedFlowPipeline\n",
    "import time\n",
    "from transformers import CLIPProcessor\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPModel\n",
    "import os\n",
    "\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Classification model\n",
    "classification_model = utils.prepare_classifier(config)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda()\n",
    "\n",
    "exp_identifier = (\n",
    "    f'instaflow_{config.epoch_size}_{config.lr}_'\n",
    "    f\"{config.seed}_{config.number_of_prompts}_{config.early_stopping}_{config.num_of_SD_backpropagation_steps}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    ")\n",
    "\n",
    "#### Train ####\n",
    "print(f\"Start experiment {exp_identifier}\")\n",
    "\n",
    "class_name = \"15 oranges\"\n",
    "print(f\"Start training class token for {class_name}\")\n",
    "img_dir_path = f\"img/{class_name}/train\"\n",
    "if Path(img_dir_path).exists():\n",
    "    shutil.rmtree(img_dir_path)\n",
    "Path(img_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Stable model\n",
    "unet, vae, text_encoder, scheduler, tokenizer = utils.prepare_stable(config)\n",
    "\n",
    "#  Extend tokenizer and add a discriminative token ###\n",
    "class_infer = int(class_name.split()[0])\n",
    "prompt_suffix = \" \".join(class_name.lower().split(\"_\"))\n",
    "\n",
    "## Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(config.placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {config.placeholder_token}. Please pass a different\"\n",
    "        \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "    )\n",
    "\n",
    "## Get token ids for our placeholder and initializer token.\n",
    "# This code block will complain if initializer string is not a single token\n",
    "## Convert the initializer_token, placeholder_token to ids\n",
    "token_ids = tokenizer.encode(config.initializer_token, add_special_tokens=False)\n",
    "# Check if initializer_token is a single token or a sequence of tokens\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(config.placeholder_token)\n",
    "\n",
    "# we resize the token embeddings here to account for placeholder_token\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#  Initialise the newly added placeholder token\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n",
    "\n",
    "# Define dataloades\n",
    "\n",
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    texts = [example[\"instance_prompt\"] for example in examples]\n",
    "    batch = {\n",
    "        \"texts\": texts,\n",
    "        \"input_ids\": input_ids,\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = prompt_dataset.PromptDataset(\n",
    "    prompt_suffix=prompt_suffix,\n",
    "    tokenizer=tokenizer,\n",
    "    placeholder_token=config.placeholder_token,\n",
    "    number_of_prompts=config.number_of_prompts,\n",
    "    epoch_size=config.epoch_size,\n",
    ")\n",
    "\n",
    "train_batch_size = config.batch_size\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Define optimization\n",
    "\n",
    "## Freeze vae and unet\n",
    "utils.freeze_params(vae.parameters())\n",
    "utils.freeze_params(unet.parameters())\n",
    "\n",
    "## Freeze all parameters except for the token embeddings in text encoder\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "utils.freeze_params(params_to_freeze)\n",
    "\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer = optimizer_class(\n",
    "    text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
    "    lr=config.lr,\n",
    "    betas=config.betas,\n",
    "    weight_decay=config.weight_decay,\n",
    "    eps=config.eps,\n",
    ")\n",
    "criterion = torch.nn.MSELoss().cuda() # TODO ozzafar torch.nn.L1Loss?\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "\n",
    "if config.gradient_checkpointing:\n",
    "    text_encoder.gradient_checkpointing_enable()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
    "    text_encoder, optimizer, train_dataloader\n",
    ")\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move vae and unet to device\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "unet.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "classification_model = classification_model.to(accelerator.device)\n",
    "text_encoder = text_encoder.to(accelerator.device)\n",
    "\n",
    "# Keep vae in eval mode as we don't train it\n",
    "vae.eval()\n",
    "# Keep unet in train mode to enable gradient checkpointing\n",
    "unet.train()\n",
    "\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "min_loss = 99999\n",
    "\n",
    "# Define token output dir\n",
    "token_dir_path = f\"token/{class_name}\"\n",
    "Path(token_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "token_path = f\"{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "\n",
    "#### Training loop ####\n",
    "train_start = time.time()\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    generator = torch.Generator(\n",
    "        device=config.device\n",
    "    )  # Seed generator to create the inital latent noise\n",
    "    generator.manual_seed(config.seed)\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        step_start = time.time()\n",
    "        # setting the generator here means we update the same images\n",
    "        classification_loss = None\n",
    "        with accelerator.accumulate(text_encoder):\n",
    "\n",
    "            generator.manual_seed(config.seed)\n",
    "            \n",
    "            # pipeline.text_encoder = text_encoder\n",
    "            pipeline = RectifiedFlowPipeline.from_pretrained(\n",
    "                \"XCLIU/instaflow_0_9B_from_sd_1_5\",\n",
    "                safety_checker = None,\n",
    "                torch_dtype=weight_dtype,\n",
    "                text_encoder=text_encoder,\n",
    "                vae=vae,\n",
    "                unet=unet,\n",
    "                tokenizer=tokenizer,\n",
    "                scheduler=scheduler,\n",
    "            ).to(device)\n",
    "            \n",
    "            # generate image            \n",
    "            image = pipeline(prompt=batch['texts'][0],\n",
    "                num_inference_steps=1,\n",
    "                height=config.height,\n",
    "                width=config.width,\n",
    "                generator=generator,\n",
    "                guidance_scale=0.0\n",
    "            ).images[0] \n",
    "\n",
    "            image = image.unsqueeze(0)\n",
    "            image_out = image\n",
    "            image = utils.transform_img_tensor(image, config).to(device)\n",
    "            \n",
    "            prompt = [class_name.split()[-1]]\n",
    "            \n",
    "            generated_image = time.time()\n",
    "            print(f\"generated image in {(generated_image - step_start) / 60} minutes\")\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                orig_output = classification_model.forward(image, prompt)\n",
    "            \n",
    "            clip_count_end = time.time()\n",
    "            print(f\"clip count executed in {(clip_count_end - generated_image)/60} minutes\")\n",
    "    \n",
    "            output_old = torch.sum(orig_output[0].detach()/config.scale)\n",
    "            pred_density1 = orig_output[0]\n",
    "            pred_density1 = pred_density1/pred_density1.max()\n",
    "            mask = torch.sigmoid(100 * (pred_density1.unsqueeze(0) - 0.5)) # TODO was 0.7\n",
    "            mask_max = F.max_pool2d(mask, kernel_size=5, stride=5)\n",
    "            mask_max = mask_max.squeeze()\n",
    "            \n",
    "            dfs(mask_max)\n",
    "            \n",
    "            output = mask_max.sum()\n",
    "            \n",
    "            if classification_loss is None:\n",
    "                classification_loss = criterion(\n",
    "                    output, torch.HalfTensor([class_infer]).cuda()\n",
    "                )/torch.HalfTensor([1]).cuda() # TODO removed power 2\n",
    "            else:\n",
    "                classification_loss += criterion(\n",
    "                    output, torch.HalfTensor([class_infer]).cuda()\n",
    "                )/torch.HalfTensor([1]).cuda() # TODO removed power 2\n",
    "                            \n",
    "            text_inputs = processor(text=prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "            inputs = {**text_inputs, \"pixel_values\": image}\n",
    "            clip_output = (clip(**inputs)[0][0]/100).cuda()\n",
    "            clip_output = _lambda * (1-clip_output)\n",
    "            \n",
    "            clip_end = time.time()\n",
    "            print(f\"clip executed in {(clip_end - clip_count_end)/60} minutes\")\n",
    "\n",
    "            classification_loss += clip_output\n",
    "\n",
    "            total_loss += classification_loss.detach().item()\n",
    "\n",
    "            # log\n",
    "            txt = f\"On epoch {epoch} \\n\"\n",
    "            with torch.no_grad():\n",
    "                txt += f\"{batch['texts']} \\n\"\n",
    "                txt += f\"{output.item()=} \\n\"\n",
    "                txt += f\"{output_old.item()=} \\n\"\n",
    "                txt += f\"Loss: {classification_loss.detach().item()} \\n\"\n",
    "                txt += f\"Clip-Count loss: {classification_loss.detach().item()-clip_output.detach().item()} \\n\"\n",
    "                txt += f\"Clip loss: {clip_output.detach().item()}\"\n",
    "                with open(\"run_log.txt\", \"a\") as f:\n",
    "                    print(txt, file=f)\n",
    "                print(txt)\n",
    "                display(utils.numpy_to_pil(\n",
    "                    image_out.detach().permute(0, 2, 3, 1).cpu().numpy()\n",
    "                )[0])\n",
    "                \n",
    "                # counting prediction heatmap\n",
    "                pred_density = orig_output[0].detach().cpu().numpy()\n",
    "                pred_density = pred_density/pred_density.max()\n",
    "                pred_density_write = 1. - pred_density\n",
    "                pred_density_write = cv2.applyColorMap(np.uint8(255*pred_density_write), cv2.COLORMAP_JET)\n",
    "                pred_density_write = pred_density_write/255.\n",
    "                img = TF.resize(image.detach(), (384)).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                heatmap_pred = 0.33 * img + 0.67 * pred_density_write\n",
    "                heatmap_pred = heatmap_pred/heatmap_pred.max()\n",
    "                display(utils.numpy_to_pil(\n",
    "                    heatmap_pred\n",
    "                )[0])\n",
    "                \n",
    "                log_end = time.time()\n",
    "                print(f\"logging in {(log_end - clip_count_end)/60} minutes\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                text_encoder.get_input_embeddings().parameters(),\n",
    "                config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            start_back = time.time()\n",
    "            \n",
    "            accelerator.backward(classification_loss)\n",
    "            \n",
    "            print(f\"backprog in {(time.time() - start_back)/60} minutes\")\n",
    "\n",
    "            # Zero out the gradients for all token embeddings except the newly added\n",
    "            # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "            if accelerator.num_processes > 1:\n",
    "                grads = (\n",
    "                    text_encoder.module.get_input_embeddings().weight.grad\n",
    "                )\n",
    "            else:\n",
    "                grads = text_encoder.get_input_embeddings().weight.grad\n",
    "\n",
    "            # Get the index for tokens that we want to zero the grads for\n",
    "            index_grads_to_zero = (\n",
    "                torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "            )\n",
    "            grads.data[index_grads_to_zero, :] = grads.data[\n",
    "                index_grads_to_zero, :\n",
    "            ].fill_(0)\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\\n\",\n",
    "            if step == config.epoch_size - 1:\n",
    "                if total_loss > 2 * min_loss:\n",
    "                    print(\"!!!!training collapse, try different hp!!!!\")\n",
    "                    # epoch = config.num_train_epochs\n",
    "                    # break\n",
    "                print(\"update\")\n",
    "                if total_loss < min_loss:\n",
    "                    min_loss = total_loss\n",
    "                    current_early_stopping = config.early_stopping\n",
    "                    # Create the pipeline using the trained modules and save it.\n",
    "                    accelerator.wait_for_everyone()\n",
    "                    if accelerator.is_main_process:\n",
    "                        print(\n",
    "                            f\"Saved the new discriminative class token pipeline of {class_name} to pipeline_{token_path}\"\n",
    "                        )\n",
    "                        pipeline.save_pretrained(f\"pipeline_{token_path}\") # TODO unwrap text encoder accelerator\n",
    "                else:\n",
    "                    current_early_stopping -= 1\n",
    "                print(\n",
    "                    f\"{current_early_stopping} steps to stop, current best {min_loss}\"\n",
    "                )\n",
    "\n",
    "                total_loss = 0\n",
    "                global_step += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    if current_early_stopping < 0:\n",
    "        break\n",
    "        \n",
    "print(f\"End train time: {(time.time()-train_start)} minutes\")"
   ],
   "metadata": {
    "id": "ASKlY8U83LdM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e323fc445422434a88336c4eeb5d423b",
      "b79a3b4a2ed84b15828ad59b029a5fdd",
      "d2fc80860b234546bc5c0067c585fe91",
      "2c1bcd22eedd4c67be4e962005482dfd",
      "36b4457dc0614826882aa714f02dcfe4",
      "eb6a1537f6f1401a9d95f8f665440daf",
      "685f8f88ebed426f97a8d1a3c3799b5e",
      "874f536d05074145af6a1f866ccfc80d",
      "b71b767e97e844c1bd3aed1c3a9a456e",
      "379887bc2d3b4547a9bca6fea6071db2",
      "094033e01e174fd1881f64d4f5359f00"
     ]
    },
    "outputId": "d191037d-b38b-4590-876c-6596222c14b1",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Eval"
   ],
   "metadata": {
    "id": "DRWHrWCOQtHS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Evaluation - print image with discriminatory tokens, then one without.\")\n",
    "# Stable model\n",
    "token_dir_path = f\"token/{class_name}\"\n",
    "Path(token_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "pipe_path = f\"pipeline_{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "pipe = RectifiedFlowPipeline.from_pretrained(pipe_path, safety_checker = None, torch_dtype=weight_dtype).to(config.device)\n",
    "\n",
    "print(f\"{pipe_path=}\")\n",
    "\n",
    "correct = dict()\n",
    "correct['a'] = 0\n",
    "correct[config.placeholder_token] = 0\n",
    "\n",
    "generator = torch.Generator(device=config.device)  # Seed generator to create the initial latent noise\n",
    "\n",
    "for descriptive_token in [config.placeholder_token,\"a\"]:\n",
    "  generator.manual_seed(config.seed)\n",
    "  prompt = f\"A photo of {descriptive_token} {class_name}\"\n",
    "  print(f\"Evaluation for the prompt: {prompt}\")\n",
    "\n",
    "  image_out = pipe(prompt, generator=generator, num_inference_steps=1, height=config.height, width=config.width, guidance_scale=0.0)[0]\n",
    "  image = utils.transform_img_tensor(image_out, config)\n",
    "\n",
    "  # output = classification_model(image).logits\n",
    "  # pred_class = torch.argmax(output).item()\n",
    "\n",
    "  display(utils.numpy_to_pil(image_out.permute(0, 2, 3, 1).cpu().detach().numpy())[0])"
   ],
   "metadata": {
    "id": "ZIXFnn1LAkTh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ]
}
