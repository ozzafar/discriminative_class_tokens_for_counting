{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100",
   "collapsed_sections": [
    "Ki8KIdXG3VG8",
    "DRWHrWCOQtHS"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e323fc445422434a88336c4eeb5d423b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b79a3b4a2ed84b15828ad59b029a5fdd",
       "IPY_MODEL_d2fc80860b234546bc5c0067c585fe91",
       "IPY_MODEL_2c1bcd22eedd4c67be4e962005482dfd"
      ],
      "layout": "IPY_MODEL_36b4457dc0614826882aa714f02dcfe4"
     }
    },
    "b79a3b4a2ed84b15828ad59b029a5fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb6a1537f6f1401a9d95f8f665440daf",
      "placeholder": "​",
      "style": "IPY_MODEL_685f8f88ebed426f97a8d1a3c3799b5e",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "d2fc80860b234546bc5c0067c585fe91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_874f536d05074145af6a1f866ccfc80d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b71b767e97e844c1bd3aed1c3a9a456e",
      "value": 7
     }
    },
    "2c1bcd22eedd4c67be4e962005482dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_379887bc2d3b4547a9bca6fea6071db2",
      "placeholder": "​",
      "style": "IPY_MODEL_094033e01e174fd1881f64d4f5359f00",
      "value": " 7/7 [00:01&lt;00:00,  6.59it/s]"
     }
    },
    "36b4457dc0614826882aa714f02dcfe4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb6a1537f6f1401a9d95f8f665440daf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "685f8f88ebed426f97a8d1a3c3799b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "874f536d05074145af6a1f866ccfc80d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b71b767e97e844c1bd3aed1c3a9a456e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "379887bc2d3b4547a9bca6fea6071db2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "094033e01e174fd1881f64d4f5359f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44UK7LWAqzJb",
    "outputId": "6c7626b0-af2e-4cca-8b89-056daaa99eb5",
    "ExecuteTime": {
     "end_time": "2024-02-17T16:08:37.249465Z",
     "start_time": "2024-02-17T16:08:37.242778900Z"
    }
   },
   "outputs": [],
   "source": [
    "#@title Setup (may take a few minutes)\n",
    "import os\n",
    "\n",
    "\n",
    "# !pip install accelerate\n",
    "# !pip install diffusers\n",
    "# !pip install transformers\n",
    "# !pip install kornia==0.6.11\n",
    "# !pip install pyrallis==0.3.1\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.utils.checkpoint\n",
    "import itertools\n",
    "from accelerate import Accelerator\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import prompt_dataset\n",
    "import utils\n",
    "from inet_classes import IDX2NAME as IDX2NAME_INET\n",
    "\n",
    "import shutil\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "#Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters"
   ],
   "metadata": {
    "id": "cm-dEEZDQ6Ba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class_index: int = 283 #@param {type:\"number\"}\n",
    "\n",
    "sd_2_1: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "classifier: str = 'clip' #@param ['inet', 'inat', 'cub','clip','clip-count'] {type:\"string\"}\n",
    "\n",
    "# Affect training time\n",
    "early_stopping: int = 15 #@param {type:\"integer\"}\n",
    "num_train_epochs: int = 50 #@param {type:\"integer\"}\n",
    "\n",
    "# affect variability of the training images\n",
    "# i.e., also sets batch size with accumulation\n",
    "epoch_size: int = 5 #@param {type:\"integer\"}\n",
    "number_of_prompts: int = 3 #@param {type:\"integer\"}\n",
    "batch_size: int = 1 #@param {type:\"integer\"}\n",
    "gradient_accumulation_steps: int = 5 #@param {type:\"integer\"}\n",
    "\n",
    "# Skip if there exists a token checkpoint\n",
    "skip_exists: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Train and Optimization\n",
    "lr: float = 0.00125 #@param {type:\"number\"}\n",
    "betas1: tuple = 0.9 #@param {type:\"number\"}\n",
    "betas2: tuple = 0.999 #@param {type:\"number\"}\n",
    "betas = (betas1, betas2)\n",
    "\n",
    "\n",
    "weight_decay: float = 1e-2 #@param {type:\"number\"}\n",
    "eps: float = 1e-08 #@param {type:\"number\"}\n",
    "max_grad_norm: str = \"1\" #@param {type:\"string\"}\n",
    "seed: int = 35 #@param {type:\"integer\"}\n",
    "\n",
    "# Generative model\n",
    "guidance_scale: int = 7 #@param {type:\"integer\"}\n",
    "height: int = 512 #@param {type:\"integer\"}\n",
    "width: int = 512 #@param {type:\"integer\"}\n",
    "num_of_SD_inference_steps: int = 35 #@param {type:\"integer\"}\n",
    "\n",
    "# Discriminative tokens\n",
    "placeholder_token: str = \"newclas\" #@param {type:\"string\"}\n",
    "initializer_token: str = \"a\" #@param {type:\"string\"}\n",
    "\n",
    "# Path to save all outputs to\n",
    "output_path: str = \"results\" #@param {type:\"string\"}\n",
    "save_as_full_pipeline: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Cuda related\n",
    "device: str = \"cuda\" #@param {type:\"string\"}\n",
    "mixed_precision: str = \"fp16\" #@param [\"fp16\", \"fp32\"] {type:\"string\"}\n",
    "gradient_checkpointing: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# evaluate\n",
    "test_size: int = 10 #@param {type:\"integer\"}"
   ],
   "metadata": {
    "id": "uY7BoAjxz5LD",
    "ExecuteTime": {
     "end_time": "2024-02-17T16:08:38.057904100Z",
     "start_time": "2024-02-17T16:08:38.056309600Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "# Define the configuration names\n",
    "config_names = [\n",
    "    \"class_index\",\n",
    "    \"sd_2_1\",\n",
    "    \"classifier\",\n",
    "    \"early_stopping\",\n",
    "    \"num_train_epochs\",\n",
    "    \"epoch_size\",\n",
    "    \"number_of_prompts\",\n",
    "    \"batch_size\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"skip_exists\",\n",
    "    \"betas\",\n",
    "    \"lr\",\n",
    "    \"eps\",\n",
    "    \"weight_decay\",\n",
    "    \"seed\",\n",
    "    \"max_grad_norm\",\n",
    "    \"guidance_scale\",\n",
    "    \"height\",\n",
    "    \"width\",\n",
    "    \"num_of_SD_inference_steps\",\n",
    "    \"placeholder_token\",\n",
    "    \"initializer_token\",\n",
    "    \"output_path\",\n",
    "    \"save_as_full_pipeline\",\n",
    "    \"device\",\n",
    "    \"mixed_precision\",\n",
    "    \"gradient_checkpointing\",\n",
    "    \"test_size\"\n",
    "]\n",
    "\n",
    "# Use globals() to extract values from matching variable names\n",
    "config_values = [globals()[name] for name in config_names]\n",
    "\n",
    "# Create the named tuple\n",
    "Config = namedtuple(\"Config\", config_names)\n",
    "config = Config(*config_values)\n",
    "config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR2klWJI9c-a",
    "outputId": "614c9805-210c-40db-d56b-e0d576faf618",
    "ExecuteTime": {
     "end_time": "2024-02-17T16:08:38.437936Z",
     "start_time": "2024-02-17T16:08:38.407279200Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Config(class_index=283, sd_2_1=False, classifier='clip', early_stopping=15, num_train_epochs=50, epoch_size=5, number_of_prompts=3, batch_size=1, gradient_accumulation_steps=5, skip_exists=False, betas=(0.9, 0.999), lr=0.00125, eps=1e-08, weight_decay=0.01, seed=35, max_grad_norm='1', guidance_scale=7, height=512, width=512, num_of_SD_inference_steps=35, placeholder_token='newclas', initializer_token='a', output_path='results', save_as_full_pipeline=True, device='cuda', mixed_precision='fp16', gradient_checkpointing=True, test_size=10)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "id": "Ki8KIdXG3VG8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-17T16:08:39.298632400Z",
     "start_time": "2024-02-17T16:08:39.298632400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import CLIPProcessor\n",
    "\n",
    "# Classification model\n",
    "classification_model = utils.prepare_classifier(config)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "exp_identifier = (\n",
    "    f'{\"2.1\" if config.sd_2_1 else \"1.4\"}_{config.epoch_size}_{config.lr}_'\n",
    "    f\"{config.seed}_{config.number_of_prompts}_{config.early_stopping}\"\n",
    ")\n",
    "\n",
    "if config.classifier == \"inet\":\n",
    "    IDX2NAME = IDX2NAME_INET\n",
    "else:\n",
    "    IDX2NAME = classification_model.config.id2label\n",
    "\n",
    "#### Train ####\n",
    "print(f\"Start experiment {exp_identifier}\")\n",
    "\n",
    "class_name = \"five cats\"\n",
    "print(f\"Start training class token for {class_name}\")\n",
    "img_dir_path = f\"img/{class_name}/train\"\n",
    "if Path(img_dir_path).exists():\n",
    "    shutil.rmtree(img_dir_path)\n",
    "Path(img_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Stable model\n",
    "unet, vae, text_encoder, scheduler, tokenizer = utils.prepare_stable(config)\n",
    "\n",
    "#  Extend tokenizer and add a discriminative token ###\n",
    "class_infer = 1\n",
    "prompt_suffix = \" \".join(class_name.lower().split(\"_\"))\n",
    "\n",
    "## Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(config.placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {config.placeholder_token}. Please pass a different\"\n",
    "        \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "    )\n",
    "\n",
    "## Get token ids for our placeholder and initializer token.\n",
    "# This code block will complain if initializer string is not a single token\n",
    "## Convert the initializer_token, placeholder_token to ids\n",
    "token_ids = tokenizer.encode(config.initializer_token, add_special_tokens=False)\n",
    "# Check if initializer_token is a single token or a sequence of tokens\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(config.placeholder_token)\n",
    "\n",
    "# we resize the token embeddings here to account for placeholder_token\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#  Initialise the newly added placeholder token\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n",
    "\n",
    "# Define dataloades\n",
    "\n",
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    texts = [example[\"instance_prompt\"] for example in examples]\n",
    "    batch = {\n",
    "        \"texts\": texts,\n",
    "        \"input_ids\": input_ids,\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = prompt_dataset.PromptDataset(\n",
    "    prompt_suffix=prompt_suffix,\n",
    "    tokenizer=tokenizer,\n",
    "    placeholder_token=config.placeholder_token,\n",
    "    number_of_prompts=config.number_of_prompts,\n",
    "    epoch_size=config.epoch_size,\n",
    ")\n",
    "\n",
    "train_batch_size = config.batch_size\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Define optimization\n",
    "\n",
    "## Freeze vae and unet\n",
    "utils.freeze_params(vae.parameters())\n",
    "utils.freeze_params(unet.parameters())\n",
    "\n",
    "## Freeze all parameters except for the token embeddings in text encoder\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "utils.freeze_params(params_to_freeze)\n",
    "\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer = optimizer_class(\n",
    "    text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
    "    lr=config.lr,\n",
    "    betas=config.betas,\n",
    "    weight_decay=config.weight_decay,\n",
    "    eps=config.eps,\n",
    ")\n",
    "criterion = torch.nn.BCELoss().cuda()\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "\n",
    "if config.gradient_checkpointing:\n",
    "    text_encoder.gradient_checkpointing_enable()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
    "    text_encoder, optimizer, train_dataloader\n",
    ")\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move vae and unet to device\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "unet.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "classification_model = classification_model.to(accelerator.device)\n",
    "text_encoder = text_encoder.to(accelerator.device)\n",
    "\n",
    "# Keep vae in eval mode as we don't train it\n",
    "vae.eval()\n",
    "# Keep unet in train mode to enable gradient checkpointing\n",
    "unet.train()\n",
    "\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "min_loss = 99999\n",
    "\n",
    "# Define token output dir\n",
    "token_dir_path = f\"token/{class_name}\"\n",
    "Path(token_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "token_path = f\"{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "\n",
    "latents_shape = (\n",
    "    config.batch_size,\n",
    "    unet.config.in_channels,\n",
    "    config.height // 8,\n",
    "    config.width // 8,\n",
    ")\n",
    "\n",
    "\n",
    "#### Training loop ####\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    generator = torch.Generator(\n",
    "        device=config.device\n",
    "    )  # Seed generator to create the inital latent noise\n",
    "    generator.manual_seed(config.seed)\n",
    "    correct = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # setting the generator here means we update the same images\n",
    "        classification_loss = None\n",
    "        with accelerator.accumulate(text_encoder):\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "            # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "            # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "            # corresponds to doing no classifier free guidance.\n",
    "            do_classifier_free_guidance = config.guidance_scale > 1.0\n",
    "\n",
    "            # get unconditional embeddings for classifier free guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                max_length = batch[\"input_ids\"].shape[-1]\n",
    "                uncond_input = tokenizer(\n",
    "                    [\"\"] * config.batch_size,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                uncond_embeddings = text_encoder(\n",
    "                    uncond_input.input_ids.to(config.device)\n",
    "                )[0]\n",
    "\n",
    "                # For classifier free guidance, we need to do two forward passes.\n",
    "                # Here we concatenate the unconditional and text embeddings into\n",
    "                # a single batch to avoid doing two forward passes.\n",
    "                encoder_hidden_states = torch.cat(\n",
    "                    [uncond_embeddings, encoder_hidden_states]\n",
    "                )\n",
    "            encoder_hidden_states = encoder_hidden_states.to(\n",
    "                dtype=weight_dtype\n",
    "            )\n",
    "            init_latent = torch.randn(\n",
    "                latents_shape, generator=generator, device=\"cuda\"\n",
    "            ).to(dtype=weight_dtype)\n",
    "\n",
    "            latents = init_latent\n",
    "            scheduler.set_timesteps(config.num_of_SD_inference_steps)\n",
    "            grad_update_step = config.num_of_SD_inference_steps - 1\n",
    "\n",
    "            # generate image\n",
    "            for i, t in enumerate(scheduler.timesteps):\n",
    "                if i < grad_update_step:  # update only partial\n",
    "                    with torch.no_grad():\n",
    "                        latent_model_input = (\n",
    "                            torch.cat([latents] * 2)\n",
    "                            if do_classifier_free_guidance\n",
    "                            else latents\n",
    "                        )\n",
    "                        noise_pred = unet(\n",
    "                            latent_model_input,\n",
    "                            t,\n",
    "                            encoder_hidden_states=encoder_hidden_states,\n",
    "                        ).sample\n",
    "\n",
    "                        # perform guidance\n",
    "                        if do_classifier_free_guidance:\n",
    "                            (\n",
    "                                noise_pred_uncond,\n",
    "                                noise_pred_text,\n",
    "                            ) = noise_pred.chunk(2)\n",
    "                            noise_pred = (\n",
    "                                noise_pred_uncond\n",
    "                                + config.guidance_scale\n",
    "                                * (noise_pred_text - noise_pred_uncond)\n",
    "                            )\n",
    "\n",
    "                        latents = scheduler.step(\n",
    "                            noise_pred, t, latents\n",
    "                        ).prev_sample\n",
    "                else:\n",
    "                    latent_model_input = (\n",
    "                        torch.cat([latents] * 2)\n",
    "                        if do_classifier_free_guidance\n",
    "                        else latents\n",
    "                    )\n",
    "                    noise_pred = unet(\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                    ).sample\n",
    "                    # perform guidance\n",
    "                    if do_classifier_free_guidance:\n",
    "                        (\n",
    "                            noise_pred_uncond,\n",
    "                            noise_pred_text,\n",
    "                        ) = noise_pred.chunk(2)\n",
    "                        noise_pred = (\n",
    "                            noise_pred_uncond\n",
    "                            + config.guidance_scale\n",
    "                            * (noise_pred_text - noise_pred_uncond)\n",
    "                        )\n",
    "\n",
    "                    latents = scheduler.step(\n",
    "                        noise_pred, t, latents\n",
    "                    ).prev_sample\n",
    "                    # scale and decode the image latents with vae\n",
    "\n",
    "            latents_decode = 1 / 0.18215 * latents\n",
    "            image = vae.decode(latents_decode).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "            image_out = image\n",
    "\n",
    "            image = utils.transform_img_tensor(image, config) # TODO ozzafar - need?\n",
    "            \n",
    "            text_inputs = processor(text=[class_name], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "            inputs = {**text_inputs, \"pixel_values\": image}\n",
    "            \n",
    "            output = (classification_model(**inputs)[0][0]/100).cuda()\n",
    "\n",
    "            if classification_loss is None:\n",
    "                classification_loss = criterion(\n",
    "                    output, torch.FloatTensor([class_infer]).cuda()\n",
    "                )\n",
    "            else:\n",
    "                classification_loss += criterion(\n",
    "                    output, torch.FloatTensor([class_infer]).cuda()\n",
    "                )\n",
    "\n",
    "            total_loss += classification_loss.detach().item()\n",
    "\n",
    "            # log\n",
    "            txt = f\"On epoch {epoch} \\n\"\n",
    "            with torch.no_grad():\n",
    "                txt += f\"{batch['texts']} \\n\"\n",
    "                txt += f\"{output.item()=} \\n\"\n",
    "                txt += f\"Loss: {classification_loss.detach().item()}\"\n",
    "                with open(\"run_log.txt\", \"a\") as f:\n",
    "                    print(txt, file=f)\n",
    "                print(txt)\n",
    "                display(utils.numpy_to_pil(\n",
    "                    image_out.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "                )[0])\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                text_encoder.get_input_embeddings().parameters(),\n",
    "                config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            accelerator.backward(classification_loss)\n",
    "\n",
    "            # Zero out the gradients for all token embeddings except the newly added\n",
    "            # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "            if accelerator.num_processes > 1:\n",
    "                grads = (\n",
    "                    text_encoder.module.get_input_embeddings().weight.grad\n",
    "                )\n",
    "            else:\n",
    "                grads = text_encoder.get_input_embeddings().weight.grad\n",
    "\n",
    "            # Get the index for tokens that we want to zero the grads for\n",
    "            index_grads_to_zero = (\n",
    "                torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "            )\n",
    "            grads.data[index_grads_to_zero, :] = grads.data[\n",
    "                index_grads_to_zero, :\n",
    "            ].fill_(0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                if total_loss > 2 * min_loss:\n",
    "                    print(\"!!!!training collapse, try different hp!!!!\")\n",
    "                    epoch = config.num_train_epochs\n",
    "                    break\n",
    "                print(\"update\")\n",
    "                if total_loss < min_loss:\n",
    "                    min_loss = total_loss\n",
    "                    current_early_stopping = config.early_stopping\n",
    "                    # Create the pipeline using the trained modules and save it.\n",
    "                    accelerator.wait_for_everyone()\n",
    "                    if accelerator.is_main_process:\n",
    "                        print(\n",
    "                            f\"Saved the new discriminative class token pipeline of {class_name} to pipeline_{token_path}\"\n",
    "                        )\n",
    "                        if config.sd_2_1:\n",
    "                            pretrained_model_name_or_path = (\n",
    "                                \"stabilityai/stable-diffusion-2-1-base\"\n",
    "                            )\n",
    "                        else:\n",
    "                            pretrained_model_name_or_path = (\n",
    "                                \"CompVis/stable-diffusion-v1-4\"\n",
    "                            )\n",
    "                        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                            pretrained_model_name_or_path,\n",
    "                            text_encoder=accelerator.unwrap_model(\n",
    "                                text_encoder\n",
    "                            ),\n",
    "                            vae=vae,\n",
    "                            unet=unet,\n",
    "                            tokenizer=tokenizer,\n",
    "                        )\n",
    "                        pipeline.save_pretrained(f\"pipeline_{token_path}\")\n",
    "                else:\n",
    "                    current_early_stopping -= 1\n",
    "                print(\n",
    "                    f\"{current_early_stopping} steps to stop, current best {min_loss}\"\n",
    "                )\n",
    "\n",
    "                total_loss = 0\n",
    "                global_step += 1\n",
    "    print(f\"Current accuracy {correct / config.epoch_size}\")\n",
    "\n",
    "    if (correct / config.epoch_size > 0.7) or current_early_stopping < 0:\n",
    "        break"
   ],
   "metadata": {
    "id": "ASKlY8U83LdM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e323fc445422434a88336c4eeb5d423b",
      "b79a3b4a2ed84b15828ad59b029a5fdd",
      "d2fc80860b234546bc5c0067c585fe91",
      "2c1bcd22eedd4c67be4e962005482dfd",
      "36b4457dc0614826882aa714f02dcfe4",
      "eb6a1537f6f1401a9d95f8f665440daf",
      "685f8f88ebed426f97a8d1a3c3799b5e",
      "874f536d05074145af6a1f866ccfc80d",
      "b71b767e97e844c1bd3aed1c3a9a456e",
      "379887bc2d3b4547a9bca6fea6071db2",
      "094033e01e174fd1881f64d4f5359f00"
     ]
    },
    "outputId": "d191037d-b38b-4590-876c-6596222c14b1",
    "ExecuteTime": {
     "end_time": "2024-02-17T16:16:30.809705800Z",
     "start_time": "2024-02-17T16:08:46.891539Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment 1.4_5_0.00125_35_3_15\n",
      "Start training class token for five cats\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e275c0d9262f464b9331617bcf5d17a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access '__len__' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n",
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'AutoencoderKL' object attribute is deprecated. Please access '__len__' over 'AutoencoderKL's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n",
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access '__len__' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n",
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'AutoencoderKL' object attribute is deprecated. Please access '__len__' over 'AutoencoderKL's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n",
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access '__len__' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n",
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'AutoencoderKL' object attribute is deprecated. Please access '__len__' over 'AutoencoderKL's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 293\u001B[0m\n\u001B[0;32m    287\u001B[0m inputs \u001B[38;5;241m=\u001B[39m processor(text\u001B[38;5;241m=\u001B[39m[class_name], images\u001B[38;5;241m=\u001B[39mimage, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    289\u001B[0m \u001B[38;5;66;03m# classifier_res = classification_model(**inputs)\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;66;03m# from torch.nn.functional import cosine_similarity\u001B[39;00m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;66;03m# output = cosine_similarity(classifier_res.image_embeds, classifier_res.text_embeds).item()\u001B[39;00m\n\u001B[1;32m--> 293\u001B[0m output \u001B[38;5;241m=\u001B[39m (classification_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m100\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m    295\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m classification_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    296\u001B[0m     classification_loss \u001B[38;5;241m=\u001B[39m criterion(\n\u001B[0;32m    297\u001B[0m         output, torch\u001B[38;5;241m.\u001B[39mFloatTensor([class_infer])\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m    298\u001B[0m     )\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[1;32mC:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Eval"
   ],
   "metadata": {
    "id": "DRWHrWCOQtHS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Evaluation - print image with discriminatory tokens, then one without.\")\n",
    "# Stable model\n",
    "token_dir_path = f\"token/{class_name}\"\n",
    "Path(token_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "pipe_path = f\"pipeline_{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(pipe_path).to(config.device)\n",
    "\n",
    "class_index = config.class_index - 1\n",
    "\n",
    "correct = dict()\n",
    "correct['a'] = 0\n",
    "correct[config.placeholder_token] = 0\n",
    "\n",
    "for seed in range(config.test_size):\n",
    "    generator = torch.Generator(device=config.device)  # Seed generator to create the inital latent noise\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    for descriptive_token in [config.placeholder_token, \"a\"]:\n",
    "      prompt = f\"A photo of {descriptive_token} {prompt_suffix}\"\n",
    "      print(f\"Evaluation for the prompt: {prompt}\")\n",
    "\n",
    "      image_out = pipe(prompt, output_type='pt', generator=generator)[0]\n",
    "      image = utils.transform_img_tensor(image_out, config)\n",
    "\n",
    "      output = classification_model(image).logits\n",
    "      pred_class = torch.argmax(output).item()\n",
    "\n",
    "      display(utils.numpy_to_pil(image_out.permute(0, 2, 3, 1).cpu().detach().numpy())[0])\n",
    "\n",
    "      if pred_class == class_index:\n",
    "          correct[descriptive_token] += 1\n",
    "      print(f\"Image class: {IDX2NAME[pred_class]}\")\n",
    "for descriptive_token in [config.placeholder_token, \"\"]:\n",
    "\n",
    "  acc = correct[descriptive_token] / config.test_size\n",
    "  print(f\"-----------------------Accuracy {descriptive_token} {acc}-----------------------------\")"
   ],
   "metadata": {
    "id": "ZIXFnn1LAkTh"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
