{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100",
   "collapsed_sections": [
    "Ki8KIdXG3VG8",
    "DRWHrWCOQtHS"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e323fc445422434a88336c4eeb5d423b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b79a3b4a2ed84b15828ad59b029a5fdd",
       "IPY_MODEL_d2fc80860b234546bc5c0067c585fe91",
       "IPY_MODEL_2c1bcd22eedd4c67be4e962005482dfd"
      ],
      "layout": "IPY_MODEL_36b4457dc0614826882aa714f02dcfe4"
     }
    },
    "b79a3b4a2ed84b15828ad59b029a5fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb6a1537f6f1401a9d95f8f665440daf",
      "placeholder": "​",
      "style": "IPY_MODEL_685f8f88ebed426f97a8d1a3c3799b5e",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "d2fc80860b234546bc5c0067c585fe91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_874f536d05074145af6a1f866ccfc80d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b71b767e97e844c1bd3aed1c3a9a456e",
      "value": 7
     }
    },
    "2c1bcd22eedd4c67be4e962005482dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_379887bc2d3b4547a9bca6fea6071db2",
      "placeholder": "​",
      "style": "IPY_MODEL_094033e01e174fd1881f64d4f5359f00",
      "value": " 7/7 [00:01&lt;00:00,  6.59it/s]"
     }
    },
    "36b4457dc0614826882aa714f02dcfe4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb6a1537f6f1401a9d95f8f665440daf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "685f8f88ebed426f97a8d1a3c3799b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "874f536d05074145af6a1f866ccfc80d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b71b767e97e844c1bd3aed1c3a9a456e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "379887bc2d3b4547a9bca6fea6071db2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "094033e01e174fd1881f64d4f5359f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44UK7LWAqzJb",
    "outputId": "6c7626b0-af2e-4cca-8b89-056daaa99eb5",
    "ExecuteTime": {
     "end_time": "2024-03-03T12:11:20.125125200Z",
     "start_time": "2024-03-03T12:11:17.114969300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozzafar\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\ozzafar\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#@title Setup (may take a few minutes)\n",
    "import os\n",
    "\n",
    "\n",
    "# !pip install accelerate\n",
    "# !pip install diffusers\n",
    "# !pip install transformers\n",
    "# !pip install kornia==0.6.11\n",
    "# !pip install pyrallis==0.3.1\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.utils.checkpoint\n",
    "import itertools\n",
    "from accelerate import Accelerator\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import prompt_dataset\n",
    "import utils\n",
    "from inet_classes import IDX2NAME as IDX2NAME_INET\n",
    "\n",
    "import shutil\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "#Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters"
   ],
   "metadata": {
    "id": "cm-dEEZDQ6Ba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class_index: int = 283 #@param {type:\"number\"}\n",
    "\n",
    "sd_2_1: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "classifier: str = 'clip-count' #@param ['inet', 'inat', 'cub','clip','clip-count'] {type:\"string\"}\n",
    "\n",
    "# Affect training time\n",
    "early_stopping: int = 15 #@param {type:\"integer\"}\n",
    "num_train_epochs: int = 50 #@param {type:\"integer\"}\n",
    "\n",
    "# affect variability of the training images\n",
    "# i.e., also sets batch size with accumulation\n",
    "epoch_size: int = 5 #@param {type:\"integer\"}\n",
    "number_of_prompts: int = 3 #@param {type:\"integer\"}\n",
    "batch_size: int = 1 #@param {type:\"integer\"}\n",
    "gradient_accumulation_steps: int = 5 #@param {type:\"integer\"}\n",
    "\n",
    "# Skip if there exists a token checkpoint\n",
    "skip_exists: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Train and Optimization\n",
    "lr: float = 0.00125 #@param {type:\"number\"}\n",
    "betas1: tuple = 0.9 #@param {type:\"number\"}\n",
    "betas2: tuple = 0.999 #@param {type:\"number\"}\n",
    "betas = (betas1, betas2)\n",
    "\n",
    "\n",
    "weight_decay: float = 1e-2 #@param {type:\"number\"}\n",
    "eps: float = 1e-08 #@param {type:\"number\"}\n",
    "max_grad_norm: str = \"1\" #@param {type:\"string\"}\n",
    "seed: int = 35 #@param {type:\"integer\"}\n",
    "\n",
    "# Generative model\n",
    "guidance_scale: int = 7 #@param {type:\"integer\"}\n",
    "height: int = 512 #@param {type:\"integer\"}\n",
    "width: int = 512 #@param {type:\"integer\"}\n",
    "num_of_SD_inference_steps: int = 35 #@param {type:\"integer\"}\n",
    "\n",
    "# Discriminative tokens\n",
    "placeholder_token: str = \"newclas\" #@param {type:\"string\"}\n",
    "initializer_token: str = \"a\" #@param {type:\"string\"}\n",
    "\n",
    "# Path to save all outputs to\n",
    "output_path: str = \"results\" #@param {type:\"string\"}\n",
    "save_as_full_pipeline: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Cuda related\n",
    "device: str = \"cuda\" #@param {type:\"string\"}\n",
    "mixed_precision: str = \"fp16\" #@param [\"fp16\", \"fp32\"] {type:\"string\"}\n",
    "gradient_checkpointing: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# evaluate\n",
    "test_size: int = 10 #@param {type:\"integer\"}"
   ],
   "metadata": {
    "id": "uY7BoAjxz5LD",
    "ExecuteTime": {
     "end_time": "2024-03-03T12:11:20.135274100Z",
     "start_time": "2024-03-03T12:11:20.133265400Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "# Define the configuration names\n",
    "config_names = [\n",
    "    \"class_index\",\n",
    "    \"sd_2_1\",\n",
    "    \"classifier\",\n",
    "    \"early_stopping\",\n",
    "    \"num_train_epochs\",\n",
    "    \"epoch_size\",\n",
    "    \"number_of_prompts\",\n",
    "    \"batch_size\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"skip_exists\",\n",
    "    \"betas\",\n",
    "    \"lr\",\n",
    "    \"eps\",\n",
    "    \"weight_decay\",\n",
    "    \"seed\",\n",
    "    \"max_grad_norm\",\n",
    "    \"guidance_scale\",\n",
    "    \"height\",\n",
    "    \"width\",\n",
    "    \"num_of_SD_inference_steps\",\n",
    "    \"placeholder_token\",\n",
    "    \"initializer_token\",\n",
    "    \"output_path\",\n",
    "    \"save_as_full_pipeline\",\n",
    "    \"device\",\n",
    "    \"mixed_precision\",\n",
    "    \"gradient_checkpointing\",\n",
    "    \"test_size\"\n",
    "]\n",
    "\n",
    "# Use globals() to extract values from matching variable names\n",
    "config_values = [globals()[name] for name in config_names]\n",
    "\n",
    "# Create the named tuple\n",
    "Config = namedtuple(\"Config\", config_names)\n",
    "config = Config(*config_values)\n",
    "config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR2klWJI9c-a",
    "outputId": "614c9805-210c-40db-d56b-e0d576faf618",
    "ExecuteTime": {
     "end_time": "2024-03-03T12:11:20.144916100Z",
     "start_time": "2024-03-03T12:11:20.135274100Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Config(class_index=283, sd_2_1=False, classifier='clip-count', early_stopping=15, num_train_epochs=50, epoch_size=5, number_of_prompts=3, batch_size=1, gradient_accumulation_steps=5, skip_exists=False, betas=(0.9, 0.999), lr=0.00125, eps=1e-08, weight_decay=0.01, seed=35, max_grad_norm='1', guidance_scale=7, height=512, width=512, num_of_SD_inference_steps=35, placeholder_token='newclas', initializer_token='a', output_path='results', save_as_full_pipeline=True, device='cuda', mixed_precision='fp16', gradient_checkpointing=True, test_size=10)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "id": "Ki8KIdXG3VG8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T12:11:20.150953600Z",
     "start_time": "2024-03-03T12:11:20.141068400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from clip_count.util import misc\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Classification model\n",
    "classification_model = utils.prepare_classifier(config)\n",
    "\n",
    "\n",
    "exp_identifier = (\n",
    "    f'{\"2.1\" if config.sd_2_1 else \"1.4\"}_{config.epoch_size}_{config.lr}_'\n",
    "    f\"{config.seed}_{config.number_of_prompts}_{config.early_stopping}\"\n",
    ")\n",
    "\n",
    "#### Train ####\n",
    "print(f\"Start experiment {exp_identifier}\")\n",
    "\n",
    "class_name = \"30 oranges\"\n",
    "print(f\"Start training class token for {class_name}\")\n",
    "img_dir_path = f\"img/{class_name}/train\"\n",
    "if Path(img_dir_path).exists():\n",
    "    shutil.rmtree(img_dir_path)\n",
    "Path(img_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Stable model\n",
    "unet, vae, text_encoder, scheduler, tokenizer = utils.prepare_stable(config)\n",
    "\n",
    "#  Extend tokenizer and add a discriminative token ###\n",
    "class_infer = int(class_name.split()[0])\n",
    "prompt_suffix = \" \".join(class_name.lower().split(\"_\"))\n",
    "\n",
    "## Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(config.placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {config.placeholder_token}. Please pass a different\"\n",
    "        \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "    )\n",
    "\n",
    "## Get token ids for our placeholder and initializer token.\n",
    "# This code block will complain if initializer string is not a single token\n",
    "## Convert the initializer_token, placeholder_token to ids\n",
    "token_ids = tokenizer.encode(config.initializer_token, add_special_tokens=False)\n",
    "# Check if initializer_token is a single token or a sequence of tokens\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(config.placeholder_token)\n",
    "\n",
    "# we resize the token embeddings here to account for placeholder_token\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#  Initialise the newly added placeholder token\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n",
    "\n",
    "# Define dataloades\n",
    "\n",
    "def collate_fn(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "    texts = [example[\"instance_prompt\"] for example in examples]\n",
    "    batch = {\n",
    "        \"texts\": texts,\n",
    "        \"input_ids\": input_ids,\n",
    "    }\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = prompt_dataset.PromptDataset(\n",
    "    prompt_suffix=prompt_suffix,\n",
    "    tokenizer=tokenizer,\n",
    "    placeholder_token=config.placeholder_token,\n",
    "    number_of_prompts=config.number_of_prompts,\n",
    "    epoch_size=config.epoch_size,\n",
    ")\n",
    "\n",
    "train_batch_size = config.batch_size\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Define optimization\n",
    "\n",
    "## Freeze vae and unet\n",
    "utils.freeze_params(vae.parameters())\n",
    "utils.freeze_params(unet.parameters())\n",
    "\n",
    "## Freeze all parameters except for the token embeddings in text encoder\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "utils.freeze_params(params_to_freeze)\n",
    "\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer = optimizer_class(\n",
    "    text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
    "    lr=config.lr,\n",
    "    betas=config.betas,\n",
    "    weight_decay=config.weight_decay,\n",
    "    eps=config.eps,\n",
    ")\n",
    "criterion = torch.nn.MSELoss().cuda() # TODO ozzafar torch.nn.L1Loss?\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    mixed_precision=config.mixed_precision,\n",
    ")\n",
    "\n",
    "if config.gradient_checkpointing:\n",
    "    text_encoder.gradient_checkpointing_enable()\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
    "    text_encoder, optimizer, train_dataloader\n",
    ")\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "elif accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "\n",
    "# Move vae and unet to device\n",
    "vae.to(accelerator.device, dtype=weight_dtype)\n",
    "unet.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "classification_model = classification_model.to(accelerator.device)\n",
    "text_encoder = text_encoder.to(accelerator.device)\n",
    "\n",
    "# Keep vae in eval mode as we don't train it\n",
    "vae.eval()\n",
    "# Keep unet in train mode to enable gradient checkpointing\n",
    "unet.train()\n",
    "\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "min_loss = 99999\n",
    "\n",
    "# Define token output dir\n",
    "token_dir_path = f\"token/{class_name}\"\n",
    "Path(token_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "token_path = f\"{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "\n",
    "latents_shape = (\n",
    "    config.batch_size,\n",
    "    unet.config.in_channels,\n",
    "    config.height // 8,\n",
    "    config.width // 8,\n",
    ")\n",
    "\n",
    "\n",
    "#### Training loop ####\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    generator = torch.Generator(\n",
    "        device=config.device\n",
    "    )  # Seed generator to create the inital latent noise\n",
    "    generator.manual_seed(config.seed)\n",
    "    correct = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # setting the generator here means we update the same images\n",
    "        classification_loss = None\n",
    "        with accelerator.accumulate(text_encoder):\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "            # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "            # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "            # corresponds to doing no classifier free guidance.\n",
    "            do_classifier_free_guidance = config.guidance_scale > 1.0\n",
    "\n",
    "            # get unconditional embeddings for classifier free guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                max_length = batch[\"input_ids\"].shape[-1]\n",
    "                uncond_input = tokenizer(\n",
    "                    [\"\"] * config.batch_size,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                uncond_embeddings = text_encoder(\n",
    "                    uncond_input.input_ids.to(config.device)\n",
    "                )[0]\n",
    "\n",
    "                # For classifier free guidance, we need to do two forward passes.\n",
    "                # Here we concatenate the unconditional and text embeddings into\n",
    "                # a single batch to avoid doing two forward passes.\n",
    "                encoder_hidden_states = torch.cat(\n",
    "                    [uncond_embeddings, encoder_hidden_states]\n",
    "                )\n",
    "            encoder_hidden_states = encoder_hidden_states.to(\n",
    "                dtype=weight_dtype\n",
    "            )\n",
    "            init_latent = torch.randn(\n",
    "                latents_shape, generator=generator, device=\"cuda\"\n",
    "            ).to(dtype=weight_dtype)\n",
    "\n",
    "            latents = init_latent\n",
    "            scheduler.set_timesteps(config.num_of_SD_inference_steps)\n",
    "            grad_update_step = config.num_of_SD_inference_steps - 1\n",
    "\n",
    "            # generate image\n",
    "            for i, t in enumerate(scheduler.timesteps):\n",
    "                if i < grad_update_step:  # update only partial\n",
    "                    with torch.no_grad():\n",
    "                        latent_model_input = (\n",
    "                            torch.cat([latents] * 2)\n",
    "                            if do_classifier_free_guidance\n",
    "                            else latents\n",
    "                        )\n",
    "                        noise_pred = unet(\n",
    "                            latent_model_input,\n",
    "                            t,\n",
    "                            encoder_hidden_states=encoder_hidden_states,\n",
    "                        ).sample\n",
    "\n",
    "                        # perform guidance\n",
    "                        if do_classifier_free_guidance:\n",
    "                            (\n",
    "                                noise_pred_uncond,\n",
    "                                noise_pred_text,\n",
    "                            ) = noise_pred.chunk(2)\n",
    "                            noise_pred = (\n",
    "                                noise_pred_uncond\n",
    "                                + config.guidance_scale\n",
    "                                * (noise_pred_text - noise_pred_uncond)\n",
    "                            )\n",
    "\n",
    "                        latents = scheduler.step(\n",
    "                            noise_pred, t, latents\n",
    "                        ).prev_sample\n",
    "                else:\n",
    "                    latent_model_input = (\n",
    "                        torch.cat([latents] * 2)\n",
    "                        if do_classifier_free_guidance\n",
    "                        else latents\n",
    "                    )\n",
    "                    noise_pred = unet(\n",
    "                        latent_model_input,\n",
    "                        t,\n",
    "                        encoder_hidden_states=encoder_hidden_states,\n",
    "                    ).sample\n",
    "                    # perform guidance\n",
    "                    if do_classifier_free_guidance:\n",
    "                        (\n",
    "                            noise_pred_uncond,\n",
    "                            noise_pred_text,\n",
    "                        ) = noise_pred.chunk(2)\n",
    "                        noise_pred = (\n",
    "                            noise_pred_uncond\n",
    "                            + config.guidance_scale\n",
    "                            * (noise_pred_text - noise_pred_uncond)\n",
    "                        )\n",
    "\n",
    "                    latents = scheduler.step(\n",
    "                        noise_pred, t, latents\n",
    "                    ).prev_sample\n",
    "                    # scale and decode the image latents with vae\n",
    "\n",
    "            latents_decode = 1 / 0.18215 * latents\n",
    "            image = vae.decode(latents_decode).sample\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "            image_out = image\n",
    "            \n",
    "            image = utils.transform_img_tensor(image, config).float()\n",
    "\n",
    "            prompt = [class_name.split()[-1]]\n",
    "            \n",
    "            # with torch.cuda.amp.autocast():\n",
    "            output = classification_model.forward(image, prompt)     \n",
    "                \n",
    "            output = torch.sum(output[0]/60).float()\n",
    "\n",
    "            if classification_loss is None:\n",
    "                classification_loss = criterion(\n",
    "                    output, torch.FloatTensor([class_infer]).cuda()\n",
    "                )\n",
    "            else:\n",
    "                classification_loss += criterion(\n",
    "                    output, torch.FloatTensor([class_infer]).cuda()\n",
    "                )\n",
    "\n",
    "            total_loss += classification_loss.detach().item()\n",
    "\n",
    "            # log\n",
    "            txt = f\"On epoch {epoch} \\n\"\n",
    "            with torch.no_grad():\n",
    "                txt += f\"{batch['texts']} \\n\"\n",
    "                txt += f\"{output.item()=} \\n\"\n",
    "                txt += f\"Loss: {classification_loss.detach().item()}\"\n",
    "                with open(\"run_log.txt\", \"a\") as f:\n",
    "                    print(txt, file=f)\n",
    "                print(txt)\n",
    "                display(utils.numpy_to_pil(\n",
    "                    image_out.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "                )[0])\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                text_encoder.get_input_embeddings().parameters(),\n",
    "                config.max_grad_norm,\n",
    "            )\n",
    "\n",
    "            accelerator.backward(classification_loss)\n",
    "\n",
    "            # Zero out the gradients for all token embeddings except the newly added\n",
    "            # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "            if accelerator.num_processes > 1:\n",
    "                grads = (\n",
    "                    text_encoder.module.get_input_embeddings().weight.grad\n",
    "                )\n",
    "            else:\n",
    "                grads = text_encoder.get_input_embeddings().weight.grad\n",
    "\n",
    "            # Get the index for tokens that we want to zero the grads for\n",
    "            index_grads_to_zero = (\n",
    "                torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "            )\n",
    "            grads.data[index_grads_to_zero, :] = grads.data[\n",
    "                index_grads_to_zero, :\n",
    "            ].fill_(0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                if total_loss > 2 * min_loss:\n",
    "                    print(\"!!!!training collapse, try different hp!!!!\")\n",
    "                    epoch = config.num_train_epochs\n",
    "                    break\n",
    "                print(\"update\")\n",
    "                if total_loss < min_loss:\n",
    "                    min_loss = total_loss\n",
    "                    current_early_stopping = config.early_stopping\n",
    "                    # Create the pipeline using the trained modules and save it.\n",
    "                    accelerator.wait_for_everyone()\n",
    "                    if accelerator.is_main_process:\n",
    "                        print(\n",
    "                            f\"Saved the new discriminative class token pipeline of {class_name} to pipeline_{token_path}\"\n",
    "                        )\n",
    "                        if config.sd_2_1:\n",
    "                            pretrained_model_name_or_path = (\n",
    "                                \"stabilityai/stable-diffusion-2-1-base\"\n",
    "                            )\n",
    "                        else:\n",
    "                            pretrained_model_name_or_path = (\n",
    "                                \"CompVis/stable-diffusion-v1-4\"\n",
    "                            )\n",
    "                        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                            pretrained_model_name_or_path,\n",
    "                            text_encoder=accelerator.unwrap_model(\n",
    "                                text_encoder\n",
    "                            ),\n",
    "                            vae=vae,\n",
    "                            unet=unet,\n",
    "                            tokenizer=tokenizer,\n",
    "                        )\n",
    "                        pipeline.save_pretrained(f\"pipeline_{token_path}\")\n",
    "                else:\n",
    "                    current_early_stopping -= 1\n",
    "                print(\n",
    "                    f\"{current_early_stopping} steps to stop, current best {min_loss}\"\n",
    "                )\n",
    "\n",
    "                total_loss = 0\n",
    "                global_step += 1\n",
    "    print(f\"Current accuracy {correct / config.epoch_size}\")\n",
    "\n",
    "    if (correct / config.epoch_size > 0.7) or current_early_stopping < 0:\n",
    "        break"
   ],
   "metadata": {
    "id": "ASKlY8U83LdM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e323fc445422434a88336c4eeb5d423b",
      "b79a3b4a2ed84b15828ad59b029a5fdd",
      "d2fc80860b234546bc5c0067c585fe91",
      "2c1bcd22eedd4c67be4e962005482dfd",
      "36b4457dc0614826882aa714f02dcfe4",
      "eb6a1537f6f1401a9d95f8f665440daf",
      "685f8f88ebed426f97a8d1a3c3799b5e",
      "874f536d05074145af6a1f866ccfc80d",
      "b71b767e97e844c1bd3aed1c3a9a456e",
      "379887bc2d3b4547a9bca6fea6071db2",
      "094033e01e174fd1881f64d4f5359f00"
     ]
    },
    "outputId": "d191037d-b38b-4590-876c-6596222c14b1",
    "ExecuteTime": {
     "end_time": "2024-03-03T12:12:18.122033800Z",
     "start_time": "2024-03-03T12:11:20.446413900Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment 1.4_5_0.00125_35_3_15\n",
      "Start training class token for 30 oranges\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ed6ed06efd340bab9def4cea9bbe012"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access '__len__' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n",
      "C:\\Program Files (x86)\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_xml.py:340: FutureWarning: Accessing config attribute `__len__` directly via 'AutoencoderKL' object attribute is deprecated. Please access '__len__' over 'AutoencoderKL's config object instead, e.g. 'unet.config.__len__'.\n",
      "  elif hasattr(v, '__len__') and not is_string(v):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and Half",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 282\u001B[0m\n\u001B[0;32m    279\u001B[0m prompt \u001B[38;5;241m=\u001B[39m [class_name\u001B[38;5;241m.\u001B[39msplit()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]]\n\u001B[0;32m    281\u001B[0m \u001B[38;5;66;03m# with torch.cuda.amp.autocast():\u001B[39;00m\n\u001B[1;32m--> 282\u001B[0m output \u001B[38;5;241m=\u001B[39m classification_model(image, prompt)     \n\u001B[0;32m    284\u001B[0m output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(output[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m60\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m classification_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\discriminative_class_tokens_for_counting\\clip_count\\models\\clip_count_model.py:196\u001B[0m, in \u001B[0;36mCLIPCount.forward\u001B[1;34m(self, imgs, text, return_extra, coop_require_grad)\u001B[0m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 196\u001B[0m         text_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_encoder(text_token)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m    198\u001B[0m cls_token, img_feat_patches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_visual_encoder(imgs, text_embedding)\n\u001B[0;32m    199\u001B[0m pred_density, extra_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_decoder(img_feat_patches, text_embedding, cls_token)  \u001B[38;5;66;03m# [N, 384, 384]\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\discriminative_class_tokens_for_counting\\clip_count\\models\\clip_count_model.py:366\u001B[0m, in \u001B[0;36mCLIPTextTransformer.forward\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    364\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_model\u001B[38;5;241m.\u001B[39mpositional_embedding\u001B[38;5;241m.\u001B[39mtype(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_model\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    365\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# NLD -> LND\u001B[39;00m\n\u001B[1;32m--> 366\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_model\u001B[38;5;241m.\u001B[39mtransformer(x)\n\u001B[0;32m    367\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# LND -> NLD\u001B[39;00m\n\u001B[0;32m    368\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_model\u001B[38;5;241m.\u001B[39mln_final(x)\u001B[38;5;241m.\u001B[39mtype(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_model\u001B[38;5;241m.\u001B[39mvisual\u001B[38;5;241m.\u001B[39mconv1\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\clip\\model.py:203\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 203\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresblocks(x)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\clip\\model.py:190\u001B[0m, in \u001B[0;36mResidualAttentionBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m--> 190\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(x))\n\u001B[0;32m    191\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_2(x))\n\u001B[0;32m    192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\clip\\model.py:187\u001B[0m, in \u001B[0;36mResidualAttentionBlock.attention\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mattention\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_mask\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(x, x, x, need_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, attn_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_mask)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   1227\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1228\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1229\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1238\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   1239\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[0;32m   1240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1241\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1242\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1243\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[0;32m   1244\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias_k, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias_v, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_zero_attn,\n\u001B[0;32m   1245\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m   1246\u001B[0m         training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining,\n\u001B[0;32m   1247\u001B[0m         key_padding_mask\u001B[38;5;241m=\u001B[39mkey_padding_mask,\n\u001B[0;32m   1248\u001B[0m         need_weights\u001B[38;5;241m=\u001B[39mneed_weights,\n\u001B[0;32m   1249\u001B[0m         attn_mask\u001B[38;5;241m=\u001B[39mattn_mask,\n\u001B[0;32m   1250\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   1251\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[0;32m   1252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\functional.py:5336\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   5334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m use_separate_proj_weight:\n\u001B[0;32m   5335\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m in_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 5336\u001B[0m     q, k, v \u001B[38;5;241m=\u001B[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001B[0;32m   5337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   5338\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m q_proj_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\nn\\functional.py:4857\u001B[0m, in \u001B[0;36m_in_projection_packed\u001B[1;34m(q, k, v, w, b)\u001B[0m\n\u001B[0;32m   4854\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mis\u001B[39;00m v:\n\u001B[0;32m   4855\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m q \u001B[38;5;129;01mis\u001B[39;00m k:\n\u001B[0;32m   4856\u001B[0m         \u001B[38;5;66;03m# self-attention\u001B[39;00m\n\u001B[1;32m-> 4857\u001B[0m         proj \u001B[38;5;241m=\u001B[39m linear(q, w, b)\n\u001B[0;32m   4858\u001B[0m         \u001B[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001B[39;00m\n\u001B[0;32m   4859\u001B[0m         proj \u001B[38;5;241m=\u001B[39m proj\u001B[38;5;241m.\u001B[39munflatten(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m3\u001B[39m, E))\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 must have the same dtype, but got Float and Half"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Eval"
   ],
   "metadata": {
    "id": "DRWHrWCOQtHS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Evaluation - print image with discriminatory tokens, then one without.\")\n",
    "# Stable model\n",
    "token_dir_path = f\"token/{class_name}\"\n",
    "Path(token_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "pipe_path = f\"pipeline_{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(pipe_path).to(config.device)\n",
    "\n",
    "class_index = config.class_index - 1\n",
    "\n",
    "correct = dict()\n",
    "correct['a'] = 0\n",
    "correct[config.placeholder_token] = 0\n",
    "\n",
    "for seed in range(config.test_size):\n",
    "    generator = torch.Generator(device=config.device)  # Seed generator to create the inital latent noise\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    for descriptive_token in [config.placeholder_token, \"a\"]:\n",
    "      prompt = f\"A photo of {descriptive_token} {prompt_suffix}\"\n",
    "      print(f\"Evaluation for the prompt: {prompt}\")\n",
    "\n",
    "      image_out = pipe(prompt, output_type='pt', generator=generator)[0]\n",
    "      image = utils.transform_img_tensor(image_out, config)\n",
    "\n",
    "      output = classification_model(image).logits\n",
    "      pred_class = torch.argmax(output).item()\n",
    "\n",
    "      display(utils.numpy_to_pil(image_out.permute(0, 2, 3, 1).cpu().detach().numpy())[0])\n",
    "\n",
    "      if pred_class == class_index:\n",
    "          correct[descriptive_token] += 1\n",
    "      print(f\"Image class: {IDX2NAME[pred_class]}\")\n",
    "for descriptive_token in [config.placeholder_token, \"\"]:\n",
    "\n",
    "  acc = correct[descriptive_token] / config.test_size\n",
    "  print(f\"-----------------------Accuracy {descriptive_token} {acc}-----------------------------\")"
   ],
   "metadata": {
    "id": "ZIXFnn1LAkTh"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
