{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100",
   "collapsed_sections": [
    "Ki8KIdXG3VG8",
    "DRWHrWCOQtHS"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "e323fc445422434a88336c4eeb5d423b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b79a3b4a2ed84b15828ad59b029a5fdd",
       "IPY_MODEL_d2fc80860b234546bc5c0067c585fe91",
       "IPY_MODEL_2c1bcd22eedd4c67be4e962005482dfd"
      ],
      "layout": "IPY_MODEL_36b4457dc0614826882aa714f02dcfe4"
     }
    },
    "b79a3b4a2ed84b15828ad59b029a5fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb6a1537f6f1401a9d95f8f665440daf",
      "placeholder": "​",
      "style": "IPY_MODEL_685f8f88ebed426f97a8d1a3c3799b5e",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "d2fc80860b234546bc5c0067c585fe91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_874f536d05074145af6a1f866ccfc80d",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b71b767e97e844c1bd3aed1c3a9a456e",
      "value": 7
     }
    },
    "2c1bcd22eedd4c67be4e962005482dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_379887bc2d3b4547a9bca6fea6071db2",
      "placeholder": "​",
      "style": "IPY_MODEL_094033e01e174fd1881f64d4f5359f00",
      "value": " 7/7 [00:01&lt;00:00,  6.59it/s]"
     }
    },
    "36b4457dc0614826882aa714f02dcfe4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb6a1537f6f1401a9d95f8f665440daf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "685f8f88ebed426f97a8d1a3c3799b5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "874f536d05074145af6a1f866ccfc80d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b71b767e97e844c1bd3aed1c3a9a456e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "379887bc2d3b4547a9bca6fea6071db2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "094033e01e174fd1881f64d4f5359f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Setup (may take a few minutes)\n",
    "import os\n",
    "\n",
    "\n",
    "# !pip install accelerate\n",
    "# !pip install diffusers\n",
    "# !pip install transformers\n",
    "# !pip install kornia==0.6.11\n",
    "# !pip install pyrallis==0.3.1\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.utils.checkpoint\n",
    "import itertools\n",
    "from accelerate import Accelerator\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import prompt_dataset\n",
    "import utils\n",
    "import cv2\n",
    "from transformers import YolosImageProcessor, YolosForObjectDetection\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from insta_flow.code.pipeline_rf import RectifiedFlowPipeline\n",
    "import time\n",
    "from transformers import CLIPProcessor\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from transformers import CLIPModel\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "#Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Hyperparameters"
   ],
   "metadata": {
    "id": "cm-dEEZDQ6Ba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "classifier: str = 'clip-count' #@param ['inet', 'inat', 'cub','clip','clip-count'] {type:\"string\"}\n",
    "\n",
    "_lambda: float = 1  #@param {type:\"number\"}\n",
    "\n",
    "# Affect training time\n",
    "early_stopping: int = 10 #@param {type:\"integer\"}\n",
    "num_train_epochs: int = 50 #@param {type:\"integer\"}\n",
    "\n",
    "# affect variability of the training images\n",
    "# i.e., also sets batch size with accumulation\n",
    "epoch_size: int = 1 #@param {type:\"integer\"}\n",
    "number_of_prompts: int = 1 #@param {type:\"integer\"}\n",
    "batch_size: int = 1 #@param {type:\"integer\"}\n",
    "gradient_accumulation_steps: int = 1 #@param {type:\"integer\"}\n",
    "\n",
    "# Skip if there exists a token checkpoint\n",
    "skip_exists: bool = False #@param {type:\"boolean\"}\n",
    "\n",
    "# Train and Optimization\n",
    "lr: float = 0.00125  #@param {type:\"number\"}\n",
    "betas1: tuple = 0.9 #@param {type:\"number\"}\n",
    "betas2: tuple = 0.999 #@param {type:\"number\"}\n",
    "betas = (betas1, betas2)\n",
    "\n",
    "\n",
    "weight_decay: float = 1e-2 #@param {type:\"number\"}\n",
    "eps: float = 1e-08 #@param {type:\"number\"}\n",
    "max_grad_norm: str = \"1\" #@param {type:\"string\"}\n",
    "\n",
    "# Generative model\n",
    "guidance_scale: int = 7 #@param {type:\"integer\"}\n",
    "height: int = 512 #@param {type:\"integer\"}\n",
    "width: int = 512 #@param {type:\"integer\"}\n",
    "num_of_SD_inference_steps: int = 35 #@param {type:\"integer\"}\n",
    "num_of_SD_backpropagation_steps: int = 1 #@param {type:\"integer\"}\n",
    "\n",
    "# Discriminative tokens\n",
    "placeholder_token: str = \"newclas\" #@param {type:\"string\"}\n",
    "initializer_token: str = \"a\" #@param {type:\"string\"}\n",
    "\n",
    "# Path to save all outputs to\n",
    "output_path: str = \"results\" #@param {type:\"string\"}\n",
    "save_as_full_pipeline: bool = True #@param {type:\"boolean\"}\n",
    "\n",
    "# Cuda related\n",
    "device: str = \"cuda\" #@param {type:\"string\"}\n",
    "mixed_precision: str = \"no\" #@param [\"fp16\", \"fp32\"] {type:\"string\"}\n",
    "gradient_checkpointing: bool = True #@param {type:\"boolean\"}"
   ],
   "metadata": {
    "id": "uY7BoAjxz5LD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "# Define the configuration names\n",
    "config_names = [\n",
    "    \"classifier\",\n",
    "    \"early_stopping\",\n",
    "    \"num_train_epochs\",\n",
    "    \"epoch_size\",\n",
    "    \"number_of_prompts\",\n",
    "    \"batch_size\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"skip_exists\",\n",
    "    \"betas\",\n",
    "    \"lr\",\n",
    "    \"eps\",\n",
    "    \"weight_decay\",\n",
    "    \"max_grad_norm\",\n",
    "    \"guidance_scale\",\n",
    "    \"height\",\n",
    "    \"width\",\n",
    "    \"num_of_SD_inference_steps\",\n",
    "    \"num_of_SD_backpropagation_steps\",\n",
    "    \"placeholder_token\",\n",
    "    \"initializer_token\",\n",
    "    \"output_path\",\n",
    "    \"save_as_full_pipeline\",\n",
    "    \"device\",\n",
    "    \"mixed_precision\",\n",
    "    \"gradient_checkpointing\",\n",
    "]\n",
    "\n",
    "# Use globals() to extract values from matching variable names\n",
    "config_values = [globals()[name] for name in config_names]\n",
    "\n",
    "# Create the named tuple\n",
    "Config = namedtuple(\"Config\", config_names)\n",
    "config = Config(*config_values)\n",
    "config"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR2klWJI9c-a",
    "outputId": "614c9805-210c-40db-d56b-e0d576faf618"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "id": "Ki8KIdXG3VG8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "classification_model = utils.prepare_classifier(config)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda()\n",
    "    \n",
    "def set_seed(generator):\n",
    "    generator.manual_seed(seed)\n",
    "    \n",
    "def run_experiment(amount, clazz, scale, seed, classification_model=classification_model,clip=clip,processor=processor):\n",
    "\n",
    "    train_start = time.time()\n",
    "    \n",
    "    exp_identifier = (\n",
    "        f'instaflow_{config.epoch_size}_{config.lr}_'\n",
    "        f\"{seed}_{config.number_of_prompts}_{config.early_stopping}_{config.num_of_SD_backpropagation_steps}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    \n",
    "    path = f\"results/{clazz}_{amount}_{seed}\"\n",
    "  \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)  \n",
    "\n",
    "    #### Train ####\n",
    "    print(f\"Start experiment {exp_identifier}\")\n",
    "    \n",
    "    class_name = f\"{amount} {clazz}\"\n",
    "    print(f\"Start training class token for {class_name}\")\n",
    "    img_dir_path = f\"img/{class_name}/train\"\n",
    "    if Path(img_dir_path).exists():\n",
    "        shutil.rmtree(img_dir_path)\n",
    "    Path(img_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    # Stable model\n",
    "    unet, vae, text_encoder, scheduler, tokenizer = utils.prepare_stable(config)\n",
    "\n",
    "    #  Extend tokenizer and add a discriminative token ###\n",
    "    class_infer = int(class_name.split()[0])\n",
    "    prompt_suffix = \" \".join(class_name.lower().split(\"_\"))\n",
    "    \n",
    "    ## Add the placeholder token in tokenizer\n",
    "    num_added_tokens = tokenizer.add_tokens(config.placeholder_token)\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(\n",
    "            f\"The tokenizer already contains the token {config.placeholder_token}. Please pass a different\"\n",
    "            \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "        )\n",
    "    \n",
    "    ## Get token ids for our placeholder and initializer token.\n",
    "    # This code block will complain if initializer string is not a single token\n",
    "    ## Convert the initializer_token, placeholder_token to ids\n",
    "    token_ids = tokenizer.encode(config.initializer_token, add_special_tokens=False)\n",
    "    # Check if initializer_token is a single token or a sequence of tokens\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(\"The initializer token must be a single token.\")\n",
    "    \n",
    "    initializer_token_id = token_ids[0]\n",
    "    placeholder_token_id = tokenizer.convert_tokens_to_ids(config.placeholder_token)\n",
    "    \n",
    "    # we resize the token embeddings here to account for placeholder_token\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    #  Initialise the newly added placeholder token\n",
    "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "    token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n",
    "    \n",
    "    # Define dataloades\n",
    "    \n",
    "    def collate_fn(examples):\n",
    "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "        input_ids = tokenizer.pad(\n",
    "            {\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        texts = [example[\"instance_prompt\"] for example in examples]\n",
    "        batch = {\n",
    "            \"texts\": texts,\n",
    "            \"input_ids\": input_ids,\n",
    "        }\n",
    "    \n",
    "        return batch\n",
    "    \n",
    "    train_dataset = prompt_dataset.PromptDataset(\n",
    "        prompt_suffix=prompt_suffix,\n",
    "        tokenizer=tokenizer,\n",
    "        placeholder_token=config.placeholder_token,\n",
    "        number_of_prompts=config.number_of_prompts,\n",
    "        epoch_size=config.epoch_size,\n",
    "    )\n",
    "    \n",
    "    train_batch_size = config.batch_size\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    # Define optimization\n",
    "\n",
    "    ## Freeze vae and unet\n",
    "    utils.freeze_params(vae.parameters())\n",
    "    utils.freeze_params(unet.parameters())\n",
    "        \n",
    "    ## Freeze all parameters except for the token embeddings in text encoder\n",
    "    params_to_freeze = itertools.chain(\n",
    "        text_encoder.text_model.encoder.parameters(),\n",
    "        text_encoder.text_model.final_layer_norm.parameters(),\n",
    "        text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    "    )\n",
    "    utils.freeze_params(params_to_freeze)\n",
    "    \n",
    "    optimizer_class = torch.optim.AdamW\n",
    "    optimizer = optimizer_class(\n",
    "        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
    "        lr=config.lr,\n",
    "        betas=config.betas,\n",
    "        weight_decay=config.weight_decay,\n",
    "        eps=config.eps,\n",
    "    )\n",
    "    criterion = torch.nn.MSELoss().cuda() # TODO ozzafar torch.nn.L1Loss?\n",
    "    \n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        mixed_precision=config.mixed_precision,\n",
    "    )\n",
    "    \n",
    "    if config.gradient_checkpointing:\n",
    "        text_encoder.gradient_checkpointing_enable()\n",
    "        unet.enable_gradient_checkpointing()\n",
    "    \n",
    "    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
    "        text_encoder, optimizer, train_dataloader\n",
    "    )\n",
    "    \n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "    \n",
    "    # Move vae and unet to device\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    \n",
    "    classification_model = classification_model.to(accelerator.device)\n",
    "    text_encoder = text_encoder.to(accelerator.device)\n",
    "    \n",
    "    # Keep vae in eval mode as we don't train it\n",
    "    vae.eval()\n",
    "    # Keep unet in train mode to enable gradient checkpointing\n",
    "    unet.train()\n",
    "\n",
    "    global_step = 0\n",
    "    total_loss = 0\n",
    "    min_loss = 99999\n",
    "    \n",
    "    # Define token output dir\n",
    "    token_dir_path = f\"token/{class_name}\"\n",
    "    Path(token_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    token_path = f\"{token_dir_path}/{exp_identifier}_{class_name}\"\n",
    "    \n",
    "    #### Training loop ####\n",
    "    for epoch in range(config.num_train_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        generator = torch.Generator(\n",
    "            device=config.device\n",
    "        )  # Seed generator to create the inital latent noise\n",
    "        generator.manual_seed(seed)\n",
    "        correct = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # setting the generator here means we update the same images\n",
    "            classification_loss = None\n",
    "            with accelerator.accumulate(text_encoder):\n",
    "    \n",
    "                set_seed(generator)\n",
    "                \n",
    "                # pipeline.text_encoder = text_encoder\n",
    "                pipeline = RectifiedFlowPipeline.from_pretrained(\n",
    "                    \"XCLIU/instaflow_0_9B_from_sd_1_5\",\n",
    "                    safety_checker = None,\n",
    "                    torch_dtype=weight_dtype,\n",
    "                    text_encoder=text_encoder,\n",
    "                    vae=vae,\n",
    "                    unet=unet,\n",
    "                    tokenizer=tokenizer,\n",
    "                    scheduler=scheduler,\n",
    "                ).to(device)\n",
    "                \n",
    "                # generate image            \n",
    "                image = pipeline(prompt=batch['texts'][0],\n",
    "                    num_inference_steps=1,\n",
    "                    height=config.height,\n",
    "                    width=config.width,\n",
    "                    generator=generator,\n",
    "                    guidance_scale=0.0\n",
    "                ).images[0] \n",
    "    \n",
    "                image = image.unsqueeze(0)\n",
    "                image_out = image\n",
    "                image = utils.transform_img_tensor(image, config).to(device)\n",
    "                \n",
    "                prompt = [class_name.split()[-1]]\n",
    "                    \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    orig_output = classification_model.forward(image, prompt)\n",
    "                        \n",
    "                output = torch.sum(orig_output[0]/scale)\n",
    "            \n",
    "                if classification_loss is None:\n",
    "                    classification_loss = criterion(\n",
    "                        output, torch.HalfTensor([class_infer]).cuda()\n",
    "                    )/torch.HalfTensor([1]).cuda() # TODO removed power 2\n",
    "                else:\n",
    "                    classification_loss += criterion(\n",
    "                        output, torch.HalfTensor([class_infer]).cuda()\n",
    "                    )/torch.HalfTensor([1]).cuda() # TODO removed power 2\n",
    "                                \n",
    "                text_inputs = processor(text=prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "                inputs = {**text_inputs, \"pixel_values\": image}\n",
    "                clip_output = (clip(**inputs)[0][0]/100).cuda()\n",
    "                clip_output = _lambda * (1-clip_output)\n",
    "                    \n",
    "                classification_loss += clip_output\n",
    "    \n",
    "                total_loss += classification_loss.detach().item()\n",
    "    \n",
    "                # log\n",
    "                txt = f\"On epoch {epoch} \\n\"\n",
    "                with torch.no_grad():\n",
    "                    txt += f\"{batch['texts']} \\n\"\n",
    "                    txt += f\"{output.item()=} \\n\"\n",
    "                    txt += f\"Loss: {classification_loss.detach().item()} \\n\"\n",
    "                    txt += f\"Clip-Count loss: {classification_loss.detach().item()-clip_output.detach().item()} \\n\"\n",
    "                    txt += f\"Clip loss: {clip_output.detach().item()}\"\n",
    "                    with open(\"run_log.txt\", \"a\") as f:\n",
    "                        print(txt, file=f)\n",
    "                    print(txt)\n",
    "                    display(utils.numpy_to_pil(\n",
    "                        image_out.detach().permute(0, 2, 3, 1).cpu().numpy()\n",
    "                    )[0])\n",
    "                    \n",
    "                    # counting prediction heatmap\n",
    "                    pred_density = orig_output[0].detach().cpu().numpy()\n",
    "                    pred_density = pred_density/pred_density.max()\n",
    "                    pred_density_write = 1. - pred_density\n",
    "                    pred_density_write = cv2.applyColorMap(np.uint8(255*pred_density_write), cv2.COLORMAP_JET)\n",
    "                    pred_density_write = pred_density_write/255.\n",
    "                    img = TF.resize(image.detach(), (384)).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "                    heatmap_pred = 0.33 * img + 0.67 * pred_density_write\n",
    "                    heatmap_pred = heatmap_pred/heatmap_pred.max()\n",
    "                    display(utils.numpy_to_pil(\n",
    "                        heatmap_pred\n",
    "                    )[0])\n",
    "                        \n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    text_encoder.get_input_embeddings().parameters(),\n",
    "                    config.max_grad_norm,\n",
    "                )\n",
    "                    \n",
    "                accelerator.backward(classification_loss)\n",
    "                    \n",
    "                # Zero out the gradients for all token embeddings except the newly added\n",
    "                # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "                if accelerator.num_processes > 1:\n",
    "                    grads = (\n",
    "                        text_encoder.module.get_input_embeddings().weight.grad\n",
    "                    )\n",
    "                else:\n",
    "                    grads = text_encoder.get_input_embeddings().weight.grad\n",
    "    \n",
    "                # Get the index for tokens that we want to zero the grads for\n",
    "                index_grads_to_zero = (\n",
    "                    torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "                )\n",
    "                grads.data[index_grads_to_zero, :] = grads.data[\n",
    "                    index_grads_to_zero, :\n",
    "                ].fill_(0)\n",
    "    \n",
    "                if epoch == step == 0:\n",
    "                    img_path = f\"{path}/actual.jpg\"\n",
    "                    utils.numpy_to_pil(image_out.permute(0, 2, 3, 1).cpu().detach().numpy())[0].save(img_path, \"JPEG\")\n",
    "\n",
    "                # Checks if the accelerator has performed an optimization step behind the scenes\\n\",\n",
    "                if step == config.epoch_size - 1:\n",
    "                    if total_loss > 2 * min_loss:\n",
    "                        print(\"!!!!training collapse, try different hp!!!!\")\n",
    "                        # epoch = config.num_train_epochs\n",
    "                        # break\n",
    "                    if total_loss < min_loss:\n",
    "                        min_loss = total_loss\n",
    "                        current_early_stopping = config.early_stopping\n",
    "                        # Create the pipeline using the trained modules and save it.\n",
    "                        accelerator.wait_for_everyone()\n",
    "                        if accelerator.is_main_process:\n",
    "                            print(\n",
    "                                f\"Saved the new discriminative class token pipeline of {class_name} to pipeline_{token_path}\"\n",
    "                            )\n",
    "                            img_path = f\"{path}/optimized.jpg\"\n",
    "                            utils.numpy_to_pil(image_out.permute(0, 2, 3, 1).cpu().detach().numpy())[0].save(img_path, \"JPEG\")\n",
    "                            # pipeline.save_pretrained(f\"pipeline_{token_path}\") # TODO unwrap text encoder accelerator\n",
    "                    else:\n",
    "                        current_early_stopping -= 1\n",
    "                    print(\n",
    "                        f\"{current_early_stopping} steps to stop, current best {min_loss}\"\n",
    "                    )\n",
    "    \n",
    "                    total_loss = 0\n",
    "                    global_step += 1\n",
    "    \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        if current_early_stopping < 0:\n",
    "            break\n",
    "    \n",
    "    print(f\"End training time: {time.time()-train_start}\")\n",
    "\n",
    "    return path"
   ],
   "metadata": {
    "id": "ASKlY8U83LdM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e323fc445422434a88336c4eeb5d423b",
      "b79a3b4a2ed84b15828ad59b029a5fdd",
      "d2fc80860b234546bc5c0067c585fe91",
      "2c1bcd22eedd4c67be4e962005482dfd",
      "36b4457dc0614826882aa714f02dcfe4",
      "eb6a1537f6f1401a9d95f8f665440daf",
      "685f8f88ebed426f97a8d1a3c3799b5e",
      "874f536d05074145af6a1f866ccfc80d",
      "b71b767e97e844c1bd3aed1c3a9a456e",
      "379887bc2d3b4547a9bca6fea6071db2",
      "094033e01e174fd1881f64d4f5359f00"
     ]
    },
    "outputId": "d191037d-b38b-4590-876c-6596222c14b1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n",
    "image_processor = YolosImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "\n",
    "def evaluate_experiment(image_path, clazz):\n",
    "    \n",
    "    count = 0\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    \n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)    \n",
    "    \n",
    "    # print results\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.8, target_sizes=target_sizes)[0]\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        if model.config.id2label[label.item()] == clazz[:-1]:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "classes = [\"oranges\"]\n",
    "intervals = [(0,5),(5,10),(10,15),(15,30),(30,50)]\n",
    "scales = [90,80,70,60,60]\n",
    "seeds = [35,1]\n",
    "\n",
    "# for i in range(100):\n",
    "#     clazz = random.choice(classes)\n",
    "#     interval = random.choice(intervals)\n",
    "#     amount = random.randint(interval[0]+1,interval[1])\n",
    "#     scale = random.choice(scales)\n",
    "#     seed = random.choice(seeds)\n",
    "#     \n",
    "#     run_experiment(amount, clazz, scale, seed)\n",
    "\n",
    "for clazz in classes:\n",
    "    for i,interval in enumerate(intervals):\n",
    "        scale = scales[i]\n",
    "        for amount in range(interval[0]+1,interval[1]+1):\n",
    "            for seed in seeds:\n",
    "                run_experiment(amount, clazz, scale, seed)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['class', 'seed', 'amount', 'sd_count', 'sd_optimized_count'])\n",
    "\n",
    "# Iterate over each subfolder inside the main folder\n",
    "for subfolder in os.listdir(\"results\"):\n",
    "    # Construct the full path of the subfolder\n",
    "    subfolder_path = os.path.join(\"results\", subfolder)\n",
    "    \n",
    "    clazz, amount, seed = subfolder_path.split('_')\n",
    "    detected_actual_amount = evaluate_experiment(subfolder_path+\"/actual.jpg\", clazz)\n",
    "    detected_optimized_amount = evaluate_experiment(subfolder_path+\"/optimized.jpg\", clazz)\n",
    "\n",
    "    df = df.append((clazz, seed, amount, detected_actual_amount, detected_optimized_amount), ignore_index=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
